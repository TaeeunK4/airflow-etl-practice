import os
import boto3
import pandas as pd
from io import StringIO

def s3_upload_csv(
        local_base_path: str, file_names: list,  
        s3_folder: str, bucket_name: str,
        aws_credentials: dict = None, target_columns: list = None):
    
    # aws_credentials -> dict.get -> id, key, region
    s3_client = boto3.client(
        "s3",
        aws_access_key_id = aws_credentials.get("AWS_ACCESS_KEY_ID"),
        aws_secret_access_key = aws_credentials.get("AWS_SECRET_ACCESS_KEY"),
        region_name = aws_credentials.get("AWS_ACCESS_REGION")
    )
    print(f"ğŸš€ [Start] '{s3_folder}' í´ë”ë¡œ ì—…ë¡œë“œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # xlsx, csv files -> 1 csv file
    for file_name in file_names:
        # local file location and name
        local_path = os.path.join(local_base_path, file_name)

        if not os.path.exists(local_path):
            print(f"âŒ [Error] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {local_path}")
            continue

        try:
            # name + ext -> name/ ext
            name, ext = os.path.splitext(file_name)
            ext = ext.lower()

            df = None

            if ext == ".xlsx":
                print(f"ğŸ”„ [Converting] {file_name} -> CSV ë³€í™˜ ì¤‘...")
                # read_excel -> memory
                df = pd.read_excel(local_path, engine="openpyxl")
            elif ext == ".csv":
                if target_columns:
                    # read_csv -> memory
                    df = pd.read_csv(local_path)
                else:
                    # direct upload_csv
                    target_key = f"{s3_folder}/{file_name}"
                    s3_client.upload_file(local_path, bucket_name, target_key)
                    print(f"âœ… [Uploaded] {file_name} -> s3://{bucket_name}/{target_key}")
            else:
                print(f"âš ï¸ [Skip] ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_name}")
                continue

            if df is not None:
                if target_columns:
                    valid_cols = [col for col in target_columns if col in df.columns]
                    if valid_cols:
                        # columns filtering
                        df = df[valid_cols]
                        print(f"âœ‚ï¸ [Filter] {file_name}: {len(valid_cols)}ê°œ ì»¬ëŸ¼ë§Œ ì„ íƒë¨")
                    else:
                        print(f"âš ï¸ [Warning] ìš”ì²­í•œ ì»¬ëŸ¼ì´ íŒŒì¼ì— í•˜ë‚˜ë„ ì—†ì–´ì„œ ì „ì²´ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.")
                
                # memory -> to_csv -> s3
                csv_buffer = StringIO()
                df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")

                target_key = f"{s3_folder}/{name}.csv"
                s3_client.put_object(
                    Bucket = bucket_name,
                    Key = target_key,
                    Body = csv_buffer.getvalue()
                )
                print(f"âœ… [Uploaded] {file_name} -> s3://{bucket_name}/{target_key}")

        except Exception as e:
            print(f"âŒ [Fail] {file_name} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            # raise e
        
    print(f"âœ¨ [Done] '{s3_folder}' ì‘ì—… ì™„ë£Œ.\n")